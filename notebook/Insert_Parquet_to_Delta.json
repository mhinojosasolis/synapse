{
	"name": "Insert_Parquet_to_Delta",
	"properties": {
		"folder": {
			"name": "Common Notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devsyncymspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/dab9cf92-bd13-4888-9e53-624f4546db3c/resourceGroups/mbrs-rg-dev-copperdamart-001/providers/Microsoft.Synapse/workspaces/mbrs-synapse-dev-cym-001/bigDataPools/devsyncymspark",
				"name": "devsyncymspark",
				"type": "Spark",
				"endpoint": "https://mbrs-synapse-dev-cym-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devsyncymspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Notebook para carga de datos parquet a tablas Delta"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true,
					"tags": [
						"parameters"
					]
				},
				"source": [
					"p_nombre_tabla = 'DLB_SHIFTLOADS'\n",
					"p_tipo_carga='HISTORICO' \n",
					"p_origen ='DISPATCH'"
				],
				"attachments": null,
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#Imports\n",
					"from pyspark.sql.functions import concat_ws, sha1, col, to_date, date_format\\\n",
					"                                ,to_utc_timestamp, current_timestamp, lit\n",
					"from notebookutils import mssparkutils\n",
					"from delta.tables import *\n",
					""
				],
				"attachments": null,
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"%run Init_variables"
				],
				"attachments": null,
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#Seteo nombre_archivo_origen con el parÃ¡metro p_nombre_tabla\n",
					"if p_tipo_carga == 'INCREMENTAL':\n",
					"    nombre_archivo_origen = f'''{blobstorage_raw_path}/{p_origen.lower()}/temp/{p_nombre_tabla}_TEMP.parquet'''\n",
					"else:\n",
					"    nombre_archivo_origen = f'''{blobstorage_raw_path}/{p_origen.lower()}/history/{p_nombre_tabla}_HIS.parquet'''    "
				],
				"attachments": null,
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#Cargo Archivo con info de Tablas, Pk y Campo Date\n",
					"archivo = f'''{blobstorage_raw_path}/parametros/TABLAS_PK_DATE.csv'''\n",
					"tablas = spark.read.load(archivo, format='csv',header='True')\n",
					"tablas.createOrReplaceTempView('tablas')"
				],
				"attachments": null,
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"sql_command = f'''\n",
					"                select key, date from tablas\n",
					"                where upper(trim(table)) = upper(trim('{p_nombre_tabla}'))\n",
					"                '''\n",
					"df_tablas = spark.sql(sql_command)   \n",
					"key_cols = df_tablas.collect()[0]['key']\n",
					"date_col = df_tablas.collect()[0]['date']"
				],
				"attachments": null,
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#Cargo el archivo nuevo y el archivo historico en dataframes\r\n",
					"new_data = spark.read.load(nombre_archivo_origen, format='parquet')\r\n",
					"\r\n",
					"\r\n",
					"#Elimino columnas ExtractDate y checksum\r\n",
					"cols = new_data.columns\r\n",
					"try:\r\n",
					"  cols.remove(date_col)\r\n",
					"except:\r\n",
					"  print(\"la columna ExtractDate no existe\")\r\n",
					"\r\n",
					"try:\r\n",
					"  cols.remove('checksum')\r\n",
					"except:\r\n",
					"  print(\"la columna checksum no existe\")\r\n",
					"\r\n",
					"#Agrego columnas checksum y PartitionDate \r\n",
					"\r\n",
					"if date_col == None:\r\n",
					"  new_data = new_data.withColumn(\"checksum\", sha1(concat_ws('-',*cols))) \\\r\n",
					"                    .withColumn(\"cdc\", to_utc_timestamp(current_timestamp(), \"UTC\"))\r\n",
					"else:  \r\n",
					"  new_data = new_data.withColumn(\"checksum\", sha1(concat_ws('-',*cols)))\\\r\n",
					"        .withColumn(\"PartitionDate\", concat_ws('-', date_format(col(date_col), \"yyyy\"), date_format(col(date_col), \"MM\")))\\\r\n",
					"        .withColumn(\"cdc\", to_utc_timestamp(current_timestamp(), \"UTC\"))\r\n",
					"\r\n",
					"# creamos vistas temporales para trabajar con las novedades con SQL\r\n",
					"new_data.createOrReplaceTempView('new_data') \r\n",
					""
				],
				"attachments": null,
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#Cargo la tabla delta\n",
					"#Si no existe la creo con los datos de temp\n",
					"table_delta_path = blobstorage_proc_path+'/DataHistorica/'+p_origen.lower()+'/delta/' + p_nombre_tabla +'_HISTORICO/'\n",
					"print(table_delta_path)\n",
					"if (DeltaTable.isDeltaTable(spark, table_delta_path)):\n",
					"    deltaTable = DeltaTable.forPath(spark, table_delta_path)\n",
					"else:\n",
					"    new_data.write.format(\"delta\").save(table_delta_path)\n",
					"    spark.sql(f''' create table delta.{p_nombre_tabla}_HISTORICO using delta location '{table_delta_path}' ''')\n",
					"    deltaTable = DeltaTable.forPath(spark, table_delta_path)\n",
					"    mssparkutils.notebook.exit('fin')"
				],
				"attachments": null,
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#Seteo Fecha maxima y minima de los nuevos datos\n",
					"if date_col != None:\n",
					"    sql_command= f'''\n",
					"                    select max({date_col}) fecha_max, min({date_col}) fecha_min from new_data \n",
					"                  '''\n",
					"    df_fechas = spark.sql(sql_command)\n",
					"    fecha_max = df_fechas.collect()[0]['fecha_max']\n",
					"    fecha_max = str(fecha_max)[0:10]\n",
					"    fecha_min = df_fechas.collect()[0]['fecha_min']\n",
					"    fecha_min = str(fecha_min)[0:10]"
				],
				"attachments": null,
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"array_key = key_cols.split(',')\n",
					"array_key\n",
					"merge_command = \"\"\n",
					"for i in array_key:    \n",
					"    merge_command = merge_command + \"historico.\"+i + \" = new_data.\" +i + \" and \"\n",
					"merge_command = merge_command[:len(merge_command) - 5]\n",
					"if date_col != None:\n",
					"    merge_command = merge_command + \" and historico.\"+ date_col + \" >= '\" + fecha_min \\\n",
					"                    +\"' and historico.\"+ date_col + \" <= '\" + fecha_max + \"'\"\n",
					"    \n",
					"\n",
					"print(merge_command)"
				],
				"attachments": null,
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"deltaTable.alias(\"historico\").merge(\n",
					"    new_data.alias(\"new_data\"),\n",
					"    merge_command)\\\n",
					"    .whenMatchedUpdateAll()\\\n",
					"    .whenNotMatchedInsertAll()\\\n",
					"    .execute()"
				],
				"attachments": null,
				"execution_count": 33
			}
		]
	}
}